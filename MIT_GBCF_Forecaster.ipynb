{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current commit:  2022-02-26 23:51:16  :  Automated update \n",
      "Current commit:  2022-02-26 23:51:16  :  Automated update \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "REPO_DIR_NAME = \"/Users/snathikudi/work/COVID-19\"\n",
    "REMOTE_URL = \"https://github.com/CSSEGISandData/COVID-19.git\"\n",
    "start_date = datetime(2022, 2, 25)\n",
    "end_date = datetime(2022, 3, 1)\n",
    "predictionFilePath= \"data/predictions/predicted_us_deaths_2022-02-20.csv\"\n",
    "skip_day_predictions =  767\n",
    "model_day_offset=747\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "def init():\n",
    "    repo = git.Repo.init(REPO_DIR_NAME)    \n",
    "    repo.remotes[0].fetch()\n",
    "    repo.remotes[0].pull()\n",
    "\n",
    "def is_exists(filename, sha):\n",
    "    repo = git.Repo(REPO_DIR_NAME)\n",
    "    \"\"\"Check if a file in current commit exist.\"\"\"\n",
    "    files = repo.git.show(\"--pretty=\", \"--name-only\", sha)\n",
    "    if filename in files:\n",
    "        return True\n",
    "    \n",
    "def get_file_commits(filename, commits):\n",
    "    file_commits = []\n",
    "    for commit in commits:\n",
    "        if is_exists(filename, commit.hexsha):\n",
    "            file_commits.append(commit)\n",
    "    return file_commits\n",
    "\n",
    "def getDataFrame(filename):\n",
    "    repo = git.Repo(REPO_DIR_NAME)\n",
    "    commits = list(repo.iter_commits(\"master\"))\n",
    "    covid_file_commits = get_file_commits(filename, commits)\n",
    "\n",
    "\n",
    "    current_time =   datetime(2020, 1, 1)\n",
    "    prev_commit_datetime = current_time\n",
    "    prev_commit = covid_file_commits[0]    \n",
    "    i = 0\n",
    "    fipsArray =[]\n",
    "    for file_commit in reversed(covid_file_commits):\n",
    "        file_commit = repo.commit(file_commit.hexsha)\n",
    "        commit_datetime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_commit.committed_date))\n",
    "        commit_datetime = datetime.strptime(commit_datetime_str, '%Y-%m-%d %H:%M:%S')\n",
    "        #print(\"commit date: \", commit_datetime_str,\"prev commit\" , prev_commit_datetime)\n",
    "        if  (((commit_datetime.date() != prev_commit_datetime.date())  \n",
    "             and prev_commit_datetime.date() > start_date.date())\n",
    "             and (commit_datetime.date() <= end_date.date())) :\n",
    "            \n",
    "            if commit_datetime.date() == (date.today()):\n",
    "                prev_commit= file_commit\n",
    "                prev_commit_datetime = commit_datetime\n",
    "                \n",
    "            print(\"Current commit: \", prev_commit_datetime,\" : \",\n",
    "                 prev_commit.message.replace('\\n', ' ') )\n",
    "            # Retrieve a file from the commit tree\n",
    "            # You can use the path helper to get the file by filename \n",
    "            targetfile = prev_commit.tree / filename\n",
    "            \n",
    "            with io.BytesIO(targetfile.data_stream.read()) as f:\n",
    "                if i == 0:\n",
    "                    US_Covid_df = pd.read_csv(f)\n",
    "                    #US_Covid_df = US_Covid_df.drop(US_Covid_df[US_Covid_df[\"Province_State\"] == \"Puerto Rico\"].index)\n",
    "                    \n",
    "                    #US_Covid_df.reset_index(drop=True, inplace=True)\n",
    "                    i =  i + 1\n",
    "                else:\n",
    "                    df = pd.read_csv(f)\n",
    "                    #df = df.drop(df[df[\"Province_State\"] == \"Puerto Rico\"].index)\n",
    "                    #df.reset_index(drop=True, inplace=True)\n",
    "                    dd = (prev_commit_datetime.date() -  timedelta(days=1)).strftime('%-m/%-d/%y')\n",
    "                    US_Covid_df[dd] = 0\n",
    "                    for index, row in US_Covid_df.iterrows():\n",
    "                        val = df.loc[df['FIPS'] == row[\"FIPS\"]][dd]\n",
    "                        if type(val) is int:\n",
    "                            US_Covid_df.at[index,dd] = val\n",
    "                        else:\n",
    "                            for v in val.values:\n",
    "                                US_Covid_df.at[index,dd] = v\n",
    "                        \n",
    "                    \n",
    "        prev_commit= file_commit\n",
    "        prev_commit_datetime = commit_datetime\n",
    "    US_Covid_df = US_Covid_df.reset_index()\n",
    "    return US_Covid_df\n",
    "init()\n",
    "#Read US Covid deaths\n",
    "US_Deaths_df_all = getDataFrame('csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv')\n",
    "#Read US Covid cases\n",
    "US_Cases_df_all = getDataFrame('csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snathikudi/anaconda3/lib/python3.8/site-packages/xarray/backends/cfgrib_.py:27: UserWarning: Failed to load cfgrib - most likely there is a problem accessing the ecCodes library. Try `import cfgrib` to get the full error message\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_day: 767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snathikudi/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4296: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n",
      "/Users/snathikudi/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4296: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_day: 788\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import  plotly.express as px \n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#US_Deaths_df = US_Deaths_df_all.copy()\n",
    "#US_Cases_df = US_Cases_df_all.copy()\n",
    "US_Deaths_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "US_Deaths_df = pd.read_csv(US_Deaths_url, error_bad_lines=True)\n",
    "US_Deaths_df=US_Deaths_df.drop(columns=['UID','iso2','iso3','code3','Admin2', 'Country_Region','Lat','Long_','Combined_Key'])\n",
    "US_Deaths_df=US_Deaths_df.fillna(0)\n",
    "US_Deaths_df=US_Deaths_df.melt(id_vars=[\"FIPS\",\"Population\",\"Province_State\"], \n",
    "        var_name=\"Date\", \n",
    "        value_name=\"Value\")\n",
    "\n",
    "Confirmed_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "US_Cases_df = pd.read_csv(Confirmed_url, error_bad_lines=True)\n",
    "US_Cases_df=US_Cases_df.drop(columns=['UID','iso2','iso3','code3','Admin2', 'Province_State', 'Country_Region','Lat','Long_','Combined_Key'])\n",
    "US_Cases_df=US_Cases_df.fillna(0)#US_Cases_df.dropna(subset=['FIPS'])\n",
    "US_Cases_df=US_Cases_df.melt(id_vars=['FIPS'], \n",
    "        var_name=\"Date\", \n",
    "        value_name=\"Value\")\n",
    "\n",
    "    \n",
    "US_Cases_df.iloc[:, 1] = pd.to_datetime(US_Cases_df.iloc[:, 1])\n",
    "US_Cases_df.iloc[:, 1]  = (US_Cases_df.iloc[:, 1]  - US_Cases_df['Date'].iloc[0]).dt.days\n",
    "US_Cases_df = US_Cases_df.rename(columns={'Date': 'Days', 'Value':'Cumulative_Cases'})\n",
    "    \n",
    "\n",
    "US_Deaths_df.iloc[:, 3] = pd.to_datetime(US_Deaths_df.iloc[:, 3])\n",
    "US_Deaths_df.iloc[:, 3]  = (US_Deaths_df.iloc[:, 3]  - US_Deaths_df['Date'].iloc[0]).dt.days\n",
    "US_Deaths_df = US_Deaths_df.rename(columns={'Date': 'Days', 'Value':'Cumulative_Deaths' })\n",
    "print(\"max_day:\" , US_Deaths_df['Days'].max())\n",
    "\n",
    "US_Predicted_Deaths = pd.read_csv(predictionFilePath,usecols=['FIPS','Population','State', 'Days','Predicted_Cumulative_Deaths']) \n",
    "US_Predicted_Deaths_Future = US_Predicted_Deaths[US_Predicted_Deaths[\"Days\"] > skip_day_predictions]\n",
    "US_Predicted_Deaths_Future.rename(columns = {'State':'Province_State', \n",
    "                                      'Predicted_Cumulative_Deaths':'Cumulative_Deaths'}, inplace = True)\n",
    "US_Deaths_df = pd.concat([US_Deaths_df, US_Predicted_Deaths_Future])\n",
    "\n",
    "US_Predicted_Cases = pd.read_csv(predictionFilePath,usecols=['FIPS', 'Days','Predicted_Cumulative_Cases'])\n",
    "US_Predicted_Cases_Future = US_Predicted_Cases[US_Predicted_Cases[\"Days\"] > skip_day_predictions]\n",
    "US_Predicted_Cases_Future.rename(columns = { 'Predicted_Cumulative_Cases':'Cumulative_Cases'}, inplace = True)\n",
    "US_Cases_df = pd.concat([US_Cases_df, US_Predicted_Cases_Future])\n",
    "print(\"max_day:\" , US_Deaths_df['Days'].max())\n",
    "\n",
    "#for index, row in US_Predicted_Deaths.iterrows():\n",
    "#    print(index)\n",
    "#    US_Deaths_df = US_Deaths_df.append({\"FIPS\": row[\"FIPS\"], \"Population\":0, \"Province_State\": row[\"State\"], \"Days\": row[\"Days\"], \"Cumulative_Deaths\":row[\"Predicted_Cumulative_Deaths\"]}, ignore_index=True)\n",
    "#    US_Cases_df = US_Cases_df.append({\"FIPS\": row[\"FIPS\"], \"Days\": row[\"Days\"], \"Cumulative_Cases\" : row[\"Predicted_Cumulative_Cases\"]}, ignore_index=True) \n",
    "\n",
    "#US_Deaths_df['Weekly_Deaths'] = US_Deaths_df.groupby(['FIPS', 'Week'])['Cumulative_Deaths'].transform(lambda x: (x.iat[-1] - x.iat[0]))\n",
    "US_Deaths_df['Weekly_Deaths'] = 0\n",
    "US_Deaths_df['Weekly_Deaths_Per'] = 0.0\n",
    "US_Deaths_df['Past_Week_Cumulative_Deaths'] = 0\n",
    "US_Deaths_df = US_Deaths_df.reset_index()\n",
    "US_Deaths_df.sort_values(by=['FIPS','Days'], inplace=True)\n",
    "cdArray = US_Deaths_df['Cumulative_Deaths'].to_numpy()\n",
    "fipsArray = US_Deaths_df['FIPS'].to_numpy()\n",
    "i = 0\n",
    "j = 0\n",
    "FIPS = 0.0\n",
    "for index, row in US_Deaths_df.iterrows():\n",
    "  countyFIPS = row['FIPS']\n",
    "  if FIPS == countyFIPS: \n",
    "    if j > 6 and fipsArray[i-7] == countyFIPS :\n",
    "        previous_cum_deaths = cdArray[i-7]\n",
    "        weekly_deaths = row['Cumulative_Deaths'] - previous_cum_deaths\n",
    "        US_Deaths_df.at[index,'Weekly_Deaths'] =  weekly_deaths\n",
    "        if previous_cum_deaths  == 0:\n",
    "          previous_cum_deaths = 1\n",
    "        US_Deaths_df.at[index, 'Past_Week_Cumulative_Deaths'] = previous_cum_deaths\n",
    "        US_Deaths_df.at[index, 'Weekly_Deaths_Per'] = (weekly_deaths  * 1.00000) / (previous_cum_deaths * 1.00000)\n",
    "  else:\n",
    "    FIPS = countyFIPS\n",
    "    j = 0 \n",
    "    US_Deaths_df.at[index,'Weekly_Deaths'] = 0\n",
    "  i = i + 1\n",
    "  j = j + 1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "US_Cases_df['Weekly_Cases'] = 0\n",
    "US_Cases_df['Weekly_Cases_Per'] = 0.0\n",
    "US_Cases_df['Past_Week_Cumulative_Cases'] = 0\n",
    "US_Cases_df = US_Cases_df.reset_index()\n",
    "US_Cases_df.sort_values(by=['FIPS','Days'], inplace=True)\n",
    "cdArray = US_Cases_df['Cumulative_Cases'].to_numpy()\n",
    "fipsArray = US_Cases_df['FIPS'].to_numpy()\n",
    "i = 0\n",
    "j = 0\n",
    "for index, row in US_Cases_df.iterrows():\n",
    "  countyFIPS = row['FIPS']\n",
    "  if FIPS == countyFIPS: \n",
    "    if j > 6 and fipsArray[i-7] == countyFIPS :\n",
    "        previous_cum_cases = cdArray[i-7]\n",
    "        weekly_cases = row['Cumulative_Cases'] - previous_cum_cases\n",
    "        US_Cases_df.at[index,'Weekly_Cases'] =  weekly_cases\n",
    "        if previous_cum_cases  == 0:\n",
    "          previous_cum_cases = 1\n",
    "        US_Cases_df.at[index, 'Past_Week_Cumulative_Cases'] = previous_cum_cases\n",
    "        US_Cases_df.at[index, 'Weekly_Cases_Per'] = (weekly_cases * 100.0) / (previous_cum_cases * 1.0)\n",
    "  else:\n",
    "    FIPS = countyFIPS\n",
    "    j = 0\n",
    "    US_Cases_df.at[index,'Weekly_Cases'] = 0\n",
    "  i = i + 1\n",
    "  j = j + 1\n",
    "    \n",
    "mobility = pd.read_csv('https://raw.githubusercontent.com/COVIDExposureIndices/COVIDExposureIndices/master/dex_data/county_dex.csv')\n",
    "mobility.iloc[:, 1] = pd.to_datetime(mobility.iloc[:, 1])\n",
    "mobility.iloc[:, 1]  = (mobility.iloc[:, 1]  - mobility['date'].iloc[0]).dt.days\n",
    "mobility = mobility.rename(columns={'date': 'Days'})\n",
    "mobility[\"Days\"] = mobility[\"Days\"] + 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-2c5a314e1a7d>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  US_Deaths_Tail_Rows[\"Days\"] = US_Deaths_Tail_Rows[\"Days\"] + 7\n",
      "<ipython-input-4-2c5a314e1a7d>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  US_Deaths_Tail_Rows[\"Past_Week_Cumulative_Deaths\"] = US_Deaths_Tail_Rows[\"Cumulative_Deaths\"]\n",
      "<ipython-input-4-2c5a314e1a7d>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  US_Cases_Tail_Rows[\"Days\"] = US_Cases_Tail_Rows[\"Days\"] + 7\n",
      "<ipython-input-4-2c5a314e1a7d>:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  US_Cases_Tail_Rows[\"Past_Week_Cumulative_Cases\"] = US_Cases_Tail_Rows[\"Cumulative_Cases\"]\n"
     ]
    }
   ],
   "source": [
    "max_days = US_Deaths_df['Days'].max()    \n",
    "US_Deaths_Tail_Rows = US_Deaths_df[US_Deaths_df[\"Days\"] > max_days - 7]\n",
    "US_Deaths_Tail_Rows[\"Days\"] = US_Deaths_Tail_Rows[\"Days\"] + 7 \n",
    "US_Deaths_Tail_Rows[\"Past_Week_Cumulative_Deaths\"] = US_Deaths_Tail_Rows[\"Cumulative_Deaths\"] \n",
    "#US_Deaths_Tail_Rows[\"Weekly_Deaths\"] = 0\n",
    "#US_Deaths_Tail_Rows[\"Weekly_Deaths_Per\"] = 0\n",
    "US_Deaths_df = pd.concat([US_Deaths_Tail_Rows, US_Deaths_df])\n",
    "US_Deaths_df.sort_values(by=['FIPS','Days'], inplace=True)\n",
    "\n",
    "#max_days = US_Deaths_df['Days'].max()    \n",
    "#US_Deaths_Tail_Rows = US_Deaths_df[US_Deaths_df[\"Days\"] > max_days - 8]\n",
    "#US_Deaths_Tail_Rows[\"Days\"] = US_Deaths_Tail_Rows[\"Days\"] + 8\n",
    "#US_Deaths_Tail_Rows[\"Cumulative_Deaths\"] = 0\n",
    "#US_Deaths_Tail_Rows[\"Past_Week_Cumulative_Deaths\"] = 0\n",
    "#US_Deaths_Tail_Rows[\"Weekly_Deaths\"] = 0\n",
    "#US_Deaths_Tail_Rows[\"Weekly_Deaths_Per\"] = 0\n",
    "#US_Deaths_df = pd.concat([US_Deaths_Tail_Rows, US_Deaths_df])\n",
    "#US_Deaths_df.sort_values(by=['FIPS','Days'], inplace=True)\n",
    "\n",
    "\n",
    "max_days = US_Cases_df['Days'].max()    \n",
    "US_Cases_Tail_Rows = US_Cases_df[US_Cases_df[\"Days\"] > max_days - 7]\n",
    "US_Cases_Tail_Rows[\"Days\"] = US_Cases_Tail_Rows[\"Days\"] + 7\n",
    "US_Cases_Tail_Rows[\"Past_Week_Cumulative_Cases\"] = US_Cases_Tail_Rows[\"Cumulative_Cases\"]\n",
    "#US_Cases_Tail_Rows[\"Weekly_Cases\"] = 0\n",
    "#US_Cases_Tail_Rows[\"Weekly_Cases_Per\"] = 0\n",
    "US_Cases_df = pd.concat([US_Cases_Tail_Rows, US_Cases_df])\n",
    "US_Cases_df.sort_values(by=['FIPS','Days'], inplace=True)\n",
    "\n",
    "#max_days = US_Cases_df['Days'].max()    \n",
    "#US_Cases_Tail_Rows = US_Cases_df[US_Cases_df[\"Days\"] > max_days - 8]\n",
    "#US_Cases_Tail_Rows[\"Days\"] = US_Cases_Tail_Rows[\"Days\"] + 8\n",
    "#US_Cases_Tail_Rows[\"Cumulative_Cases\"] = 0\n",
    "#US_Cases_Tail_Rows[\"Past_Week_Cumulative_Cases\"] = 0\n",
    "#US_Cases_Tail_Rows[\"Weekly_Cases\"] = 0\n",
    "#US_Cases_Tail_Rows[\"Weekly_Cases_Per\"] = 0\n",
    "#US_Cases_df = pd.concat([US_Cases_Tail_Rows, US_Cases_df])\n",
    "#US_Cases_df.sort_values(by=['FIPS','Days'], inplace=True)\n",
    "\n",
    "\n",
    "US_Deaths_df.to_csv(\"usdf.csv\")\n",
    "US_Cases_df.to_csv(\"uscf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvHB7ubQ8LjS"
   },
   "outputs": [],
   "source": [
    "def daterange(date1, date2):\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        yield date1 + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Population</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Days</th>\n",
       "      <th>Cumulative_Deaths</th>\n",
       "      <th>Weekly_Deaths</th>\n",
       "      <th>Weekly_Deaths_Per</th>\n",
       "      <th>Past_Week_Cumulative_Deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>1269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>1306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>1338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>1593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>488943</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>2956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Utah</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550518</th>\n",
       "      <td>2550518</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Grand Princess</td>\n",
       "      <td>763</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553860</th>\n",
       "      <td>2553860</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Grand Princess</td>\n",
       "      <td>764</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557202</th>\n",
       "      <td>2557202</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Grand Princess</td>\n",
       "      <td>765</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560544</th>\n",
       "      <td>2560544</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Grand Princess</td>\n",
       "      <td>766</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563886</th>\n",
       "      <td>2563886</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Grand Princess</td>\n",
       "      <td>767</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153600 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index     FIPS  Population  Province_State  Days  \\\n",
       "1269        1269      0.0           0   Massachusetts     0   \n",
       "1306        1306      0.0           0        Michigan     0   \n",
       "1338        1338      0.0           0        Michigan     0   \n",
       "1593        1593      0.0      488943        Missouri     0   \n",
       "2956        2956      0.0           0            Utah     0   \n",
       "...          ...      ...         ...             ...   ...   \n",
       "2550518  2550518  99999.0           0  Grand Princess   763   \n",
       "2553860  2553860  99999.0           0  Grand Princess   764   \n",
       "2557202  2557202  99999.0           0  Grand Princess   765   \n",
       "2560544  2560544  99999.0           0  Grand Princess   766   \n",
       "2563886  2563886  99999.0           0  Grand Princess   767   \n",
       "\n",
       "         Cumulative_Deaths  Weekly_Deaths  Weekly_Deaths_Per  \\\n",
       "1269                   0.0              0                0.0   \n",
       "1306                   0.0              0                0.0   \n",
       "1338                   0.0              0                0.0   \n",
       "1593                   0.0              0                0.0   \n",
       "2956                   0.0              0                0.0   \n",
       "...                    ...            ...                ...   \n",
       "2550518                3.0              0                0.0   \n",
       "2553860                3.0              0                0.0   \n",
       "2557202                3.0              0                0.0   \n",
       "2560544                3.0              0                0.0   \n",
       "2563886                3.0              0                0.0   \n",
       "\n",
       "         Past_Week_Cumulative_Deaths  \n",
       "1269                             0.0  \n",
       "1306                             0.0  \n",
       "1338                             0.0  \n",
       "1593                             0.0  \n",
       "2956                             0.0  \n",
       "...                              ...  \n",
       "2550518                          3.0  \n",
       "2553860                          3.0  \n",
       "2557202                          3.0  \n",
       "2560544                          3.0  \n",
       "2563886                          3.0  \n",
       "\n",
       "[153600 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unassigned_fips = [  0.0, 60.0,    66.0,    69.0,    78.0,  2013.0,  2261.0, 72001.0,  72003.0,  72005.0,\n",
    " 72007.0,  72009.0,  72011.0,  72013.0,  72015.0,  72017.0,  72019.0,  72021.0,  72023.0,  72025.0,\n",
    " 72027.0,  72029.0,  72031.0,  72033.0,  72035.0,  72037.0,  72039.0,  72041.0,  72043.0,  72045.0,\n",
    " 72047.0,  72049.0,  72051.0,  72053.0,  72054.0,  72055.0,  72057.0,  72059.0,  72061.0,  72063.0,\n",
    " 72065.0,  72067.0,  72069.0,  72071.0,  72073.0,  72075.0,  72077.0,  72079.0,  72081.0,  72083.0,\n",
    " 72085.0,  72087.0,  72089.0,  72091.0,  72093.0,  72095.0,  72097.0,  72099.0,  72101.0,  72103.0,\n",
    " 72105.0,  72107.0,  72109.0,  72111.0,  72113.0,  72115.0,  72117.0,  72119.0,  72121.0,  72123.0,\n",
    " 72125.0,  72127.0,  72129.0,  72131.0,  72133.0,  72135.0,  72137.0,  72139.0,  72141.0,  72143.0,\n",
    " 72145.0,  72147.0,  72149.0,  72151.0,  72153.0,  72888.0,  72999.0,  80001.0, 80002.0, 80004.0, 80005.0,\n",
    " 80006.0, 80008.0, 80009.0, 80010.0, 80011.0, 80012.0, 80013.0, 80015.0, 80016.0, 80017.0,\n",
    " 80018.0, 80019.0, 80020.0, 80021.0, 80022.0, 80023.0, 80024.0, 80025.0, 80026.0, 80027.0,\n",
    " 80028.0, 80029.0, 80030.0, 80031.0, 80032.0, 80033.0, 80034.0, 80035.0, 80036.0, 80037.0,\n",
    " 80038.0, 80039.0, 80040.0, 80041.0, 80042.0, 80044.0, 80045.0, 80046.0, 80047.0, 80048.0,\n",
    " 80049.0, 80050.0, 80051.0, 80053.0, 80054.0, 80055.0, 80056.0, 88888.0, 90001.0, 90002.0,\n",
    " 90004.0, 90005.0, 90006.0, 90008.0, 90009.0, 90010.0, 90011.0, 90012.0, 90013.0, 90015.0,\n",
    " 90016.0, 90017.0, 90018.0, 90019.0, 90020.0, 90021.0, 90022.0, 90023.0, 90024.0, 90025.0,\n",
    " 90026.0, 90027.0, 90028.0, 90029.0, 90030.0, 90031.0, 90032.0, 90033.0, 90034.0, 90035.0,\n",
    " 90036.0, 90037.0, 90038.0, 90039.0, 90040.0, 90041.0, 90042.0, 90044.0, 90045.0, 90046.0,\n",
    " 90047.0, 90048.0, 90049.0, 90050.0, 90051.0, 90053.0, 90054.0, 90055.0, 90056.0, 99999.0]\n",
    "unassigned_deaths_df = US_Deaths_df[US_Deaths_df['FIPS'].isin(unassigned_fips)]\n",
    "unassigned_deaths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjOaGxmUwynr"
   },
   "outputs": [],
   "source": [
    "#load state tests data to combine with cases and deaths\n",
    "#data is not available for all days, populate zero for missing ones\n",
    "from datetime import timedelta, date\n",
    "\n",
    "state_codes = pd.read_csv('https://docs.google.com/spreadsheets/d/1DAqxIYJdagFN85ncoTQO-CbpJLQECvZWt6qwNMQZUkk/export?format=csv')\n",
    "start_date = date(2020, 1, 22)\n",
    "end_date = date.today()\n",
    "\n",
    "state_codes_date_df = pd.DataFrame({'Date': pd.Series([], dtype='str'),\n",
    "                   'Code': pd.Series([], dtype='str'),\n",
    "                   'State': pd.Series([], dtype='str')})\n",
    "for single_date in daterange(start_date, end_date):\n",
    "  for index, row in state_codes.iterrows():\n",
    "    state_codes_date_df = state_codes_date_df.append({'Date': single_date.strftime(\"%Y%m%d\"), 'Code' : row[\"Code\"], 'State' : row[\"State\"]}, ignore_index=True)\n",
    "state_tests = pd.read_csv(\"https://covidtracking.com/api/v1/states/daily.csv\")\n",
    "convert_dict = {'date': str} \n",
    "state_tests = state_tests.astype(convert_dict)\n",
    "state_tests_df = state_codes_date_df.merge(state_tests[['date','state','positive', 'negative']],how='left', left_on=['Date', 'Code'], right_on=['date', 'state'])\n",
    "state_tests_df.sort_values(['Date','State'], inplace=True)\n",
    "state_tests_df.iloc[:, 0] = pd.to_datetime(state_tests_df.iloc[:, 0],format=\"%Y%m%d\")\n",
    "state_tests_df.iloc[:, 0]  = (state_tests_df.iloc[:, 0] - state_tests_df['Date'].iloc[0]).dt.days\n",
    "state_tests_df = state_tests_df.rename(columns={'Date': 'Days'})\n",
    "state_tests_df['Cumulative_Tests'] = state_tests_df['positive'] + state_tests_df ['negative']\n",
    "state_tests_df = state_tests_df.drop(columns=['date', 'state', 'positive', 'negative'])\n",
    "state_tests_df.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21GaGHXZT8M2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fips', 'TOT_POP', '0-9', '0-9 y/o % of total pop', '10-19',\n",
      "       '10-19 y/o % of total pop', '20-29', '20-29 y/o % of total pop',\n",
      "       '30-39', '30-39 y/o % of total pop',\n",
      "       ...\n",
      "       'Number of AGGRAVATED ASSAULTS', 'BURGLRY', 'LARCENY',\n",
      "       'MOTOR VEHICLE THEFTS', 'ARSON', 'ICU Beds_y', 'Total Population',\n",
      "       'Population Aged 60+', 'Percent of Population Aged 60+',\n",
      "       'Residents Aged 60+ Per Each ICU Bed'],\n",
      "      dtype='object', length=381)\n"
     ]
    }
   ],
   "source": [
    "#Read county variates dataset and combine with FIPS, race, icu_beds dataset\n",
    "county_covariates= pd.read_csv('https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv').dropna(subset=['FIPS'])\n",
    "county_covariates.FIPS = county_covariates.FIPS.astype('int64')\n",
    "county_covariates=county_covariates.set_index('FIPS')\n",
    "age_race_df = pd.read_csv('https://docs.google.com/spreadsheets/d/12GIRONjeNHeKFb3EKpo5r-VvdsnTwwe0iOtKBsnZVM0/export?format=csv')\n",
    "county_icu_beds= pd.read_csv('https://docs.google.com/spreadsheets/d/13iUBUwRcE91_x9FhsF8Ugcb0_tFauWJF2Z-PSkERDlU/export?format=csv')\n",
    "FIPS = pd.read_csv('https://docs.google.com/spreadsheets/d/1jUwRaTSJ__3Wp60cZLLox5u55mJTZrShtjEK4d7xTEY/export?format=csv')\n",
    "covariates = age_race_df.merge(county_covariates, how='inner', left_on=[\"fips\"], right_on=['FIPS'])\n",
    "covariates = covariates.drop(['STNAME', 'County', 'Unnamed: 0','State', 'Area_Name'], axis=1)\n",
    "#covariates = covariates.drop(['Unnamed: 0','State', 'Area_Name'], axis=1)\n",
    "\n",
    "covariates = covariates.merge(county_icu_beds, how='inner', left_on=[\"fips\"], right_on=['fips'])\n",
    "#covariates = covariates.drop(['County','State'], axis=1)\n",
    "covariates = covariates.fillna(0)\n",
    "#dropping full NaN counties\n",
    "#covariates = covariates[covariates['Jul Temp Max / F'].notnull()]\n",
    "#merge with US FIPS and makue  sure FIPS are only in the US\n",
    "US_Deaths_All_FIPS_df= US_Deaths_df.copy()\n",
    "US_Cases_All_FIPS_df= US_Cases_df.copy()\n",
    "US_Deaths_df = US_Deaths_df.merge(FIPS,how='inner', left_on=[\"FIPS\"], right_on=['fips'])\n",
    "US_Cases_df = US_Cases_df.merge(FIPS,how='inner', left_on=[\"FIPS\"], right_on=['fips'])\n",
    "covariates = covariates.dropna(axis='columns')\n",
    "list(covariates.columns)\n",
    "print(covariates.columns)\n",
    "covariates=covariates[['fips',\n",
    " 'TOT_POP',\n",
    " '0-9',\n",
    " '0-9 y/o % of total pop',\n",
    " '10-19',\n",
    " '10-19 y/o % of total pop',\n",
    " '20-29',\n",
    " '20-29 y/o % of total pop',\n",
    " '30-39',\n",
    " '30-39 y/o % of total pop',\n",
    " '40-49',\n",
    " '40-49 y/o % of total pop',\n",
    " '50-59',\n",
    " '50-59 y/o % of total pop',\n",
    " '60-69',\n",
    " '60-69 y/o % of total pop',\n",
    " '70-79',\n",
    " '70-79 y/o % of total pop',\n",
    " '80+',\n",
    " '80+ y/o % of total pop',\n",
    " 'White-alone pop',\n",
    " '% White-alone',\n",
    " 'Black-alone pop',\n",
    " '% Black-alone',\n",
    " 'Native American/American Indian-alone pop',\n",
    " '% NA/AI-alone',\n",
    " 'Asian-alone pop',\n",
    " '% Asian-alone',\n",
    " 'Hawaiian/Pacific Islander-alone pop',\n",
    " '% Hawaiian/PI-alone',\n",
    " 'Two or more races pop',\n",
    " '% Two or more races',\n",
    " 'POP_ESTIMATE_2018',\n",
    " 'N_POP_CHG_2018',\n",
    " 'GQ_ESTIMATES_2018',\n",
    " 'R_birth_2018',\n",
    " 'R_death_2018',\n",
    " 'R_NATURAL_INC_2018',\n",
    " 'R_INTERNATIONAL_MIG_2018',\n",
    " 'R_DOMESTIC_MIG_2018',\n",
    " 'R_NET_MIG_2018',\n",
    " 'Less than a high school diploma 2014-18',\n",
    " 'High school diploma only 2014-18',\n",
    " \"Some college or associate's degree 2014-18\",\n",
    " \"Bachelor's degree or higher 2014-18\",\n",
    " 'Percent of adults with less than a high school diploma 2014-18',\n",
    " 'Percent of adults with a high school diploma only 2014-18',\n",
    " \"Percent of adults completing some college or associate's degree 2014-18\",\n",
    " \"Percent of adults with a bachelor's degree or higher 2014-18\",\n",
    " 'POVALL_2018',\n",
    " 'PCTPOVALL_2018',\n",
    " 'PCTPOV017_2018',\n",
    " 'PCTPOV517_2018',\n",
    " 'MEDHHINC_2018',\n",
    " 'CI90LBINC_2018',\n",
    " 'CI90UBINC_2018',\n",
    " 'Civilian_labor_force_2018',\n",
    " 'Employed_2018',\n",
    " 'Unemployed_2018',\n",
    " 'Unemployment_rate_2018',\n",
    " 'Median_Household_Income_2018',\n",
    " 'Med_HH_Income_Percent_of_State_Total_2018',\n",
    " 'Jan Precipitation / inch',\n",
    " 'Feb Precipitation / inch',\n",
    " 'Mar Precipitation / inch',\n",
    " 'Apr Precipitation / inch',\n",
    " 'May Precipitation / inch',\n",
    " 'Jun Precipitation / inch',\n",
    " 'Jul Precipitation / inch',\n",
    " 'Jan Temp AVG / F',\n",
    " 'Feb Temp AVG / F',\n",
    " 'Mar Temp AVG / F',\n",
    " 'Apr Temp AVG / F',\n",
    " 'May Temp AVG / F',\n",
    " 'Jun Temp AVG / F',\n",
    " 'Jul Temp AVG / F',\n",
    " 'Active Physicians per 100000 Population 2018 (AAMC)',\n",
    " 'Total Active Patient Care Physicians per 100000 Population 2018 (AAMC)',\n",
    " 'Active Primary Care Physicians per 100000 Population 2018 (AAMC)',\n",
    " 'Active Patient Care Primary Care Physicians per 100000 Population 2018 (AAMC)',\n",
    " 'Active General Surgeons per 100000 Population 2018 (AAMC)',\n",
    " 'Active Patient Care General Surgeons per 100000 Population 2018 (AAMC)',\n",
    " 'Total nurse practitioners (2019)',\n",
    " 'Total physician assistants (2019)',\n",
    " 'Total Hospitals (2019)',\n",
    " 'Internal Medicine Primary Care (2019)',\n",
    " 'Family Medicine/General Practice Primary Care (2019)',\n",
    " 'Total Specialist Physicians (2019)',\n",
    " 'ICU Beds_x',\n",
    " 'Total Population',\n",
    " 'Population Aged 60+',\n",
    " 'Percent of Population Aged 60+']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VF6n1wU4ynx3"
   },
   "outputs": [],
   "source": [
    "#build prior - 2 ,3 week(s) cases and deaths as features\n",
    "\n",
    "US_Cases_Prior_Fourteen_df = US_Cases_df.copy() \n",
    "US_Cases_Prior_Fourteen_df[\"Days\"] = US_Cases_df[\"Days\"] + 14\n",
    "US_Cases_Prior_Fourteen_df.rename(columns = {'Weekly_Cases_Per':'Two_Week_Prior_Weekly_Cases_Per', 'Weekly_Cases':'Two_Week_Prior_Weekly_Cases'}, inplace = True)\n",
    "US_Cases_Prior_TwentyOne_df = US_Cases_df.copy() \n",
    "US_Cases_Prior_TwentyOne_df[\"Days\"] = US_Cases_df[\"Days\"] + 21\n",
    "US_Cases_Prior_TwentyOne_df.rename(columns = {'Weekly_Cases_Per':'Three_Week_Prior_Weekly_Cases_Per', 'Weekly_Cases':'Three_Week_Prior_Weekly_Cases'}, inplace = True)\n",
    "US_Deaths_Cases_df = US_Deaths_df.merge(US_Cases_df)\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(US_Cases_Prior_TwentyOne_df[['Three_Week_Prior_Weekly_Cases_Per', 'Three_Week_Prior_Weekly_Cases', 'FIPS', 'Days']], how='left', left_on=['FIPS', \"Days\"], right_on=['FIPS', 'Days'])\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(US_Cases_Prior_Fourteen_df[['Two_Week_Prior_Weekly_Cases_Per', 'Two_Week_Prior_Weekly_Cases', 'FIPS', 'Days']], how='left', left_on=['FIPS', \"Days\"], right_on=['FIPS', 'Days'])\n",
    "\n",
    "US_Deaths_Prior_TwentyOne_df = US_Deaths_df.copy()\n",
    "US_Deaths_Prior_TwentyOne_df[\"Days\"] = US_Deaths_df[\"Days\"] + 21\n",
    "US_Deaths_Prior_TwentyOne_df.rename(columns = {'Weekly_Deaths_Per':'Three_Week_Prior_Weekly_Deaths_Per', 'Weekly_Deaths':'Three_Week_Prior_Weekly_Deaths'}, inplace = True)\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(US_Deaths_Prior_TwentyOne_df[['Three_Week_Prior_Weekly_Deaths_Per', 'Three_Week_Prior_Weekly_Deaths', 'FIPS', 'Days']], how='left', left_on=['FIPS', \"Days\"], right_on=['FIPS', 'Days'])\n",
    "\n",
    "US_Deaths_Prior_Fourteen_df = US_Deaths_df.copy()\n",
    "US_Deaths_Prior_Fourteen_df[\"Days\"] = US_Deaths_df[\"Days\"] + 14\n",
    "US_Deaths_Prior_Fourteen_df.rename(columns = {'Weekly_Deaths_Per':'Two_Week_Prior_Weekly_Deaths_Per', 'Weekly_Deaths':'Two_Week_Prior_Weekly_Deaths'}, inplace = True)\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(US_Deaths_Prior_Fourteen_df[['Two_Week_Prior_Weekly_Deaths_Per', 'Two_Week_Prior_Weekly_Deaths', 'FIPS', 'Days']], how='left', left_on=['FIPS', \"Days\"], right_on=['FIPS', 'Days'])\n",
    "US_Deaths_Cases_df.to_csv(\"udc1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "xseYJzjKY8FK",
    "outputId": "c77e79f3-b0e5-4328-ffca-afbd3fa0cb6e"
   },
   "outputs": [],
   "source": [
    "mobility[\"county\"] = mobility[\"county\"].astype(\"float\")\n",
    "#US_Deaths_Cases_df = US_Deaths_Cases_df.merge(state_tests_df, how='left', left_on=['Province_State', \"Days\"], right_on=['State', 'Days'])\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(mobility[['dex', 'num_devices','county','Days']], how='left', left_on=['FIPS', \"Days\"], right_on=['county', 'Days'])\n",
    "\n",
    "covariates_merged = covariates.merge(US_Deaths_Cases_df[[\n",
    "                                                        'Three_Week_Prior_Weekly_Cases_Per',\n",
    "                                                        'Three_Week_Prior_Weekly_Cases',\n",
    "                                                        'Two_Week_Prior_Weekly_Cases_Per',\n",
    "                                                        'Two_Week_Prior_Weekly_Cases',\n",
    "                                                        'Three_Week_Prior_Weekly_Deaths_Per',\n",
    "                                                        'Three_Week_Prior_Weekly_Deaths',\n",
    "                                                        'Two_Week_Prior_Weekly_Deaths_Per',\n",
    "                                                        'Two_Week_Prior_Weekly_Deaths',                        \n",
    "                                                         'dex','num_devices','FIPS', 'Days',]], how='inner', left_on=[\"fips\"], right_on=['FIPS'])\n",
    "#dropping NaN columns\n",
    "covariate_merged = covariates_merged.dropna(axis='columns')\n",
    "US_Deaths_Cases_df =  US_Deaths_Cases_df.dropna(axis='columns')\n",
    "fips_state=age_race_df[['STNAME','fips']]\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(fips_state, how='inner', left_on=[\"FIPS\"], right_on=['fips'])\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.drop(columns=['index'])\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#corr=covariate_merged.corr()\n",
    "#print(corr[['Cumulative_Deaths','Daily_Deaths']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 0,\n",
       " 2: 0,\n",
       " 3: 0,\n",
       " 4: 0,\n",
       " 5: 0,\n",
       " 6: 0,\n",
       " 7: 0,\n",
       " 8: 0,\n",
       " 9: 0,\n",
       " 10: 0,\n",
       " 11: 0,\n",
       " 12: 0,\n",
       " 13: 0,\n",
       " 14: 0,\n",
       " 15: 0,\n",
       " 16: 0,\n",
       " 17: 0,\n",
       " 18: 0,\n",
       " 19: 0,\n",
       " 20: 0,\n",
       " 21: 0,\n",
       " 22: 0,\n",
       " 23: 0,\n",
       " 24: 0,\n",
       " 25: 0,\n",
       " 26: 0,\n",
       " 27: 0,\n",
       " 28: 0,\n",
       " 29: 0,\n",
       " 30: 0,\n",
       " 31: 0,\n",
       " 32: 0,\n",
       " 33: 0,\n",
       " 34: 0,\n",
       " 35: 0,\n",
       " 36: 0,\n",
       " 37: 0,\n",
       " 38: 0,\n",
       " 39: 0,\n",
       " 40: 0,\n",
       " 41: 0,\n",
       " 42: 0,\n",
       " 43: 0,\n",
       " 44: 0,\n",
       " 45: 0,\n",
       " 46: 0,\n",
       " 47: 0,\n",
       " 48: 0,\n",
       " 49: 0,\n",
       " 50: 0,\n",
       " 51: 0,\n",
       " 52: 1,\n",
       " 53: 1,\n",
       " 54: 2,\n",
       " 55: 7,\n",
       " 56: 13,\n",
       " 57: 31,\n",
       " 58: 39,\n",
       " 59: 43,\n",
       " 60: 66,\n",
       " 61: 97,\n",
       " 62: 143,\n",
       " 63: 183,\n",
       " 64: 254,\n",
       " 65: 325,\n",
       " 66: 372,\n",
       " 67: 413,\n",
       " 68: 530,\n",
       " 69: 789,\n",
       " 70: 671,\n",
       " 71: 797,\n",
       " 72: 930,\n",
       " 73: 1119,\n",
       " 74: 1311,\n",
       " 75: 1496,\n",
       " 76: 1555,\n",
       " 77: 1880,\n",
       " 78: 2004,\n",
       " 79: 2124,\n",
       " 80: 1946,\n",
       " 81: 1928,\n",
       " 82: 1953,\n",
       " 83: 1875,\n",
       " 84: 1686,\n",
       " 85: 1525,\n",
       " 86: 1362,\n",
       " 87: 1416,\n",
       " 88: 1365,\n",
       " 89: 1276,\n",
       " 90: 1412,\n",
       " 91: 1339,\n",
       " 92: 1506,\n",
       " 93: 1664,\n",
       " 94: 998,\n",
       " 95: 1070,\n",
       " 96: 703,\n",
       " 97: 366,\n",
       " 98: 403,\n",
       " 99: -57,\n",
       " 100: -398,\n",
       " 101: 179,\n",
       " 102: 38,\n",
       " 103: 287,\n",
       " 104: 210,\n",
       " 105: 342,\n",
       " 106: 533,\n",
       " 107: 623,\n",
       " 108: 543,\n",
       " 109: 477,\n",
       " 110: 390,\n",
       " 111: 462,\n",
       " 112: 300,\n",
       " 113: 237,\n",
       " 114: 171,\n",
       " 115: 217,\n",
       " 116: 200,\n",
       " 117: -3710,\n",
       " 118: -3792,\n",
       " 119: -3901,\n",
       " 120: -3975,\n",
       " 121: -3943,\n",
       " 122: -4034,\n",
       " 123: -4090,\n",
       " 124: -231,\n",
       " 125: -197,\n",
       " 126: -57,\n",
       " 127: -3,\n",
       " 128: -138,\n",
       " 129: -91,\n",
       " 130: -225,\n",
       " 131: -217,\n",
       " 132: -236,\n",
       " 133: -267,\n",
       " 134: -282,\n",
       " 135: -218,\n",
       " 136: -275,\n",
       " 137: -125,\n",
       " 138: -117,\n",
       " 139: -107,\n",
       " 140: -100,\n",
       " 141: -80,\n",
       " 142: -62,\n",
       " 143: 22,\n",
       " 144: 24,\n",
       " 145: 4,\n",
       " 146: 11,\n",
       " 147: -33,\n",
       " 148: -2,\n",
       " 149: -7,\n",
       " 150: -30,\n",
       " 151: -17,\n",
       " 152: -16,\n",
       " 153: -34,\n",
       " 154: -5,\n",
       " 155: -1884,\n",
       " 156: -1862,\n",
       " 157: -1851,\n",
       " 158: -1846,\n",
       " 159: -1853,\n",
       " 160: -1913,\n",
       " 161: -1914,\n",
       " 162: -63,\n",
       " 163: -77,\n",
       " 164: -48,\n",
       " 165: -53,\n",
       " 166: -50,\n",
       " 167: 228,\n",
       " 168: 207,\n",
       " 169: 223,\n",
       " 170: 224,\n",
       " 171: 221,\n",
       " 172: 222,\n",
       " 173: 236,\n",
       " 174: 20,\n",
       " 175: 35,\n",
       " 176: 27,\n",
       " 177: 48,\n",
       " 178: 36,\n",
       " 179: 35,\n",
       " 180: 34,\n",
       " 181: 34,\n",
       " 182: 40,\n",
       " 183: 29,\n",
       " 184: 42,\n",
       " 185: 71,\n",
       " 186: 51,\n",
       " 187: 124,\n",
       " 188: 129,\n",
       " 189: 218,\n",
       " 190: 230,\n",
       " 191: 66,\n",
       " 192: 156,\n",
       " 193: 141,\n",
       " 194: 84,\n",
       " 195: 142,\n",
       " 196: 119,\n",
       " 197: 137,\n",
       " 198: 318,\n",
       " 199: 215,\n",
       " 200: 241,\n",
       " 201: 220,\n",
       " 202: -68,\n",
       " 203: -108,\n",
       " 204: -105,\n",
       " 205: -101,\n",
       " 206: -122,\n",
       " 207: -58,\n",
       " 208: -41,\n",
       " 209: 198,\n",
       " 210: 216,\n",
       " 211: 70,\n",
       " 212: 26,\n",
       " 213: 50,\n",
       " 214: 3,\n",
       " 215: -13,\n",
       " 216: -3,\n",
       " 217: -33,\n",
       " 218: 104,\n",
       " 219: 128,\n",
       " 220: 139,\n",
       " 221: 117,\n",
       " 222: 96,\n",
       " 223: 83,\n",
       " 224: 39,\n",
       " 225: 32,\n",
       " 226: 53,\n",
       " 227: 54,\n",
       " 228: 70,\n",
       " 229: 85,\n",
       " 230: 102,\n",
       " 231: 134,\n",
       " 232: 126,\n",
       " 233: 116,\n",
       " 234: 117,\n",
       " 235: 97,\n",
       " 236: 106,\n",
       " 237: 100,\n",
       " 238: -16,\n",
       " 239: -8,\n",
       " 240: -25,\n",
       " 241: -17,\n",
       " 242: -22,\n",
       " 243: -30,\n",
       " 244: 75,\n",
       " 245: 179,\n",
       " 246: 153,\n",
       " 247: 161,\n",
       " 248: 171,\n",
       " 249: 183,\n",
       " 250: 189,\n",
       " 251: 79,\n",
       " 252: 115,\n",
       " 253: 124,\n",
       " 254: 132,\n",
       " 255: 123,\n",
       " 256: 116,\n",
       " 257: 110,\n",
       " 258: 90,\n",
       " 259: 82,\n",
       " 260: 77,\n",
       " 261: 88,\n",
       " 262: 63,\n",
       " 263: 53,\n",
       " 264: 72,\n",
       " 265: 92,\n",
       " 266: 118,\n",
       " 267: 80,\n",
       " 268: 126,\n",
       " 269: 120,\n",
       " 270: 120,\n",
       " 271: 98,\n",
       " 272: 91,\n",
       " 273: 93,\n",
       " 274: 98,\n",
       " 275: 86,\n",
       " 276: 87,\n",
       " 277: 86,\n",
       " 278: 99,\n",
       " 279: 94,\n",
       " 280: 48,\n",
       " 281: 64,\n",
       " 282: 99,\n",
       " 283: 89,\n",
       " 284: 74,\n",
       " 285: 77,\n",
       " 286: 114,\n",
       " 287: 180,\n",
       " 288: 139,\n",
       " 289: 215,\n",
       " 290: 269,\n",
       " 291: 293,\n",
       " 292: 271,\n",
       " 293: 250,\n",
       " 294: 227,\n",
       " 295: 241,\n",
       " 296: 158,\n",
       " 297: 165,\n",
       " 298: 179,\n",
       " 299: -16,\n",
       " 300: 25,\n",
       " 301: 32,\n",
       " 302: 68,\n",
       " 303: 102,\n",
       " 304: 86,\n",
       " 305: 111,\n",
       " 306: 416,\n",
       " 307: 401,\n",
       " 308: 385,\n",
       " 309: 400,\n",
       " 310: 449,\n",
       " 311: 459,\n",
       " 312: 437,\n",
       " 313: 387,\n",
       " 314: 388,\n",
       " 315: 413,\n",
       " 316: 403,\n",
       " 317: 398,\n",
       " 318: 412,\n",
       " 319: 435,\n",
       " 320: 518,\n",
       " 321: 526,\n",
       " 322: 473,\n",
       " 323: 478,\n",
       " 324: 523,\n",
       " 325: 500,\n",
       " 326: 469,\n",
       " 327: 445,\n",
       " 328: 442,\n",
       " 329: 332,\n",
       " 330: 321,\n",
       " 331: 218,\n",
       " 332: 225,\n",
       " 333: 232,\n",
       " 334: 281,\n",
       " 335: 290,\n",
       " 336: 236,\n",
       " 337: 230,\n",
       " 338: 240,\n",
       " 339: 261,\n",
       " 340: 296,\n",
       " 341: 206,\n",
       " 342: 224,\n",
       " 343: 376,\n",
       " 344: 361,\n",
       " 345: 295,\n",
       " 346: 346,\n",
       " 347: 304,\n",
       " 348: 315,\n",
       " 349: 259,\n",
       " 350: 217,\n",
       " 351: 236,\n",
       " 352: 356,\n",
       " 353: 267,\n",
       " 354: 268,\n",
       " 355: 377,\n",
       " 356: 500,\n",
       " 357: 205,\n",
       " 358: 240,\n",
       " 359: 188,\n",
       " 360: 231,\n",
       " 361: 222,\n",
       " 362: 77,\n",
       " 363: 2,\n",
       " 364: 320,\n",
       " 365: 236,\n",
       " 366: 205,\n",
       " 367: 167,\n",
       " 368: 159,\n",
       " 369: 195,\n",
       " 370: 315,\n",
       " 371: 253,\n",
       " 372: 317,\n",
       " 373: 259,\n",
       " 374: 366,\n",
       " 375: 244,\n",
       " 376: 199,\n",
       " 377: 214,\n",
       " 378: 244,\n",
       " 379: 424,\n",
       " 380: 453,\n",
       " 381: 340,\n",
       " 382: 450,\n",
       " 383: 404,\n",
       " 384: 418,\n",
       " 385: 509,\n",
       " 386: 147,\n",
       " 387: 97,\n",
       " 388: 100,\n",
       " 389: 39,\n",
       " 390: 21,\n",
       " 391: 8,\n",
       " 392: -114,\n",
       " 393: 14,\n",
       " 394: 38,\n",
       " 395: 2,\n",
       " 396: 71,\n",
       " 397: 119,\n",
       " 398: 109,\n",
       " 399: 63,\n",
       " 400: 9,\n",
       " 401: 31,\n",
       " 402: 25,\n",
       " 403: 5,\n",
       " 404: -7,\n",
       " 405: -21,\n",
       " 406: 64,\n",
       " 407: 113,\n",
       " 408: 71,\n",
       " 409: 60,\n",
       " 410: 38,\n",
       " 411: 11,\n",
       " 412: 50,\n",
       " 413: 23,\n",
       " 414: 27,\n",
       " 415: 10,\n",
       " 416: 3,\n",
       " 417: 3,\n",
       " 418: 8,\n",
       " 419: 12,\n",
       " 420: 46,\n",
       " 421: 36,\n",
       " 422: 21,\n",
       " 423: 37,\n",
       " 424: 27,\n",
       " 425: 45,\n",
       " 426: 7,\n",
       " 427: 10,\n",
       " 428: 12,\n",
       " 429: 41,\n",
       " 430: 31,\n",
       " 431: 25,\n",
       " 432: -2,\n",
       " 433: 52,\n",
       " 434: 72,\n",
       " 435: 66,\n",
       " 436: 78,\n",
       " 437: 87,\n",
       " 438: 99,\n",
       " 439: 100,\n",
       " 440: 72,\n",
       " 441: 61,\n",
       " 442: 62,\n",
       " 443: 54,\n",
       " 444: 39,\n",
       " 445: 34,\n",
       " 446: 42,\n",
       " 447: 63,\n",
       " 448: 98,\n",
       " 449: 64,\n",
       " 450: 53,\n",
       " 451: 46,\n",
       " 452: 49,\n",
       " 453: 48,\n",
       " 454: 56,\n",
       " 455: -4,\n",
       " 456: 62,\n",
       " 457: 78,\n",
       " 458: 92,\n",
       " 459: 85,\n",
       " 460: 99,\n",
       " 461: 93,\n",
       " 462: 96,\n",
       " 463: 86,\n",
       " 464: 93,\n",
       " 465: 97,\n",
       " 466: 102,\n",
       " 467: 101,\n",
       " 468: 135,\n",
       " 469: 128,\n",
       " 470: 94,\n",
       " 471: 81,\n",
       " 472: 58,\n",
       " 473: 59,\n",
       " 474: 58,\n",
       " 475: 41,\n",
       " 476: 47,\n",
       " 477: 65,\n",
       " 478: 75,\n",
       " 479: 83,\n",
       " 480: 60,\n",
       " 481: 56,\n",
       " 482: 59,\n",
       " 483: 71,\n",
       " 484: 63,\n",
       " 485: 61,\n",
       " 486: 39,\n",
       " 487: 63,\n",
       " 488: 40,\n",
       " 489: 142,\n",
       " 490: 13,\n",
       " 491: 14,\n",
       " 492: 33,\n",
       " 493: 46,\n",
       " 494: 35,\n",
       " 495: 56,\n",
       " 496: -39,\n",
       " 497: 78,\n",
       " 498: 68,\n",
       " 499: 48,\n",
       " 500: 51,\n",
       " 501: 58,\n",
       " 502: 59,\n",
       " 503: 44,\n",
       " 504: 32,\n",
       " 505: 39,\n",
       " 506: 279,\n",
       " 507: 286,\n",
       " 508: 275,\n",
       " 509: 266,\n",
       " 510: 274,\n",
       " 511: 286,\n",
       " 512: 288,\n",
       " 513: 330,\n",
       " 514: 325,\n",
       " 515: 322,\n",
       " 516: 316,\n",
       " 517: 322,\n",
       " 518: 321,\n",
       " 519: 307,\n",
       " 520: 230,\n",
       " 521: 224,\n",
       " 522: 233,\n",
       " 523: 231,\n",
       " 524: 227,\n",
       " 525: 214,\n",
       " 526: 216,\n",
       " 527: 202,\n",
       " 528: 206,\n",
       " 529: 200,\n",
       " 530: 198,\n",
       " 531: 195,\n",
       " 532: 206,\n",
       " 533: 201,\n",
       " 534: 179,\n",
       " 535: 174,\n",
       " 536: 174,\n",
       " 537: 177,\n",
       " 538: 184,\n",
       " 539: 192,\n",
       " 540: 200,\n",
       " 541: 178,\n",
       " 542: 175,\n",
       " 543: 271,\n",
       " 544: 276,\n",
       " 545: 284,\n",
       " 546: 275,\n",
       " 547: 257,\n",
       " 548: 388,\n",
       " 549: 394,\n",
       " 550: 298,\n",
       " 551: 299,\n",
       " 552: 416,\n",
       " 553: 298,\n",
       " 554: 297,\n",
       " 555: 424,\n",
       " 556: 427,\n",
       " 557: 426,\n",
       " 558: 439,\n",
       " 559: 320,\n",
       " 560: 455,\n",
       " 561: 459,\n",
       " 562: 673,\n",
       " 563: 679,\n",
       " 564: 683,\n",
       " 565: 692,\n",
       " 566: 1164,\n",
       " 567: 1172,\n",
       " 568: 1529,\n",
       " 569: 1151,\n",
       " 570: 1146,\n",
       " 571: 1146,\n",
       " 572: 1511,\n",
       " 573: 1079,\n",
       " 574: 1084,\n",
       " 575: 1534,\n",
       " 576: 1620,\n",
       " 577: 1620,\n",
       " 578: 1616,\n",
       " 579: 1741,\n",
       " 580: 1743,\n",
       " 581: 1740,\n",
       " 582: 1841,\n",
       " 583: 1877,\n",
       " 584: 1870,\n",
       " 585: 1876,\n",
       " 586: 2012,\n",
       " 587: 2018,\n",
       " 588: 1994,\n",
       " 589: 2448,\n",
       " 590: 2547,\n",
       " 591: 2540,\n",
       " 592: 2536,\n",
       " 593: 1919,\n",
       " 594: 2565,\n",
       " 595: 2588,\n",
       " 596: 2554,\n",
       " 597: 2629,\n",
       " 598: 2633,\n",
       " 599: 2627,\n",
       " 600: 3150,\n",
       " 601: 2520,\n",
       " 602: 2499,\n",
       " 603: 2744,\n",
       " 604: 2667,\n",
       " 605: 2642,\n",
       " 606: 2626,\n",
       " 607: 2783,\n",
       " 608: 2827,\n",
       " 609: 2803,\n",
       " 610: 2433,\n",
       " 611: 2473,\n",
       " 612: 2498,\n",
       " 613: 2509,\n",
       " 614: 2378,\n",
       " 615: 2746,\n",
       " 616: 2746,\n",
       " 617: 2487,\n",
       " 618: 2339,\n",
       " 619: 2284,\n",
       " 620: 2274,\n",
       " 621: 2107,\n",
       " 622: 1721,\n",
       " 623: 1713,\n",
       " 624: 1571,\n",
       " 625: 1511,\n",
       " 626: 1537,\n",
       " 627: 1540,\n",
       " 628: 1189,\n",
       " 629: 1849,\n",
       " 630: 1840,\n",
       " 631: 1468,\n",
       " 632: 1341,\n",
       " 633: 1321,\n",
       " 634: 1318,\n",
       " 635: 1631,\n",
       " 636: 967,\n",
       " 637: 985,\n",
       " 638: 1021,\n",
       " 639: 1095,\n",
       " 640: 1087,\n",
       " 641: 1087,\n",
       " 642: 915,\n",
       " 643: 894,\n",
       " 644: 875,\n",
       " 645: 964,\n",
       " 646: 922,\n",
       " 647: 919,\n",
       " 648: 924,\n",
       " 649: 872,\n",
       " 650: 837,\n",
       " 651: 840,\n",
       " 652: 265,\n",
       " 653: 775,\n",
       " 654: 750,\n",
       " 655: 747,\n",
       " 656: 791,\n",
       " 657: 888,\n",
       " 658: 876,\n",
       " 659: 867,\n",
       " 660: 1243,\n",
       " 661: 1262,\n",
       " 662: 1258,\n",
       " 663: 1361,\n",
       " 664: 1342,\n",
       " 665: 1360,\n",
       " 666: 1431,\n",
       " 667: 628,\n",
       " 668: 615,\n",
       " 669: 618,\n",
       " 670: 539,\n",
       " 671: 535,\n",
       " 672: 558,\n",
       " 673: 470,\n",
       " 674: 212,\n",
       " 675: 213,\n",
       " 676: 221,\n",
       " 677: 565,\n",
       " 678: 584,\n",
       " 679: 563,\n",
       " 680: 756,\n",
       " 681: 820,\n",
       " 682: 805,\n",
       " 683: 789,\n",
       " 684: 470,\n",
       " 685: 464,\n",
       " 686: 495,\n",
       " 687: 727,\n",
       " 688: 557,\n",
       " 689: 542,\n",
       " 690: 537,\n",
       " 691: 561,\n",
       " 692: 533,\n",
       " 693: 564,\n",
       " 694: 329,\n",
       " 695: 476,\n",
       " 696: 482,\n",
       " 697: 481,\n",
       " 698: 471,\n",
       " 699: 488,\n",
       " 700: 453,\n",
       " 701: -59,\n",
       " 702: -190,\n",
       " 703: -191,\n",
       " 704: -189,\n",
       " 705: -218,\n",
       " 706: -85,\n",
       " 707: -201,\n",
       " 708: 328,\n",
       " 709: 373,\n",
       " 710: 369,\n",
       " 711: 417,\n",
       " 712: 462,\n",
       " 713: 386,\n",
       " 714: 362,\n",
       " 715: 457,\n",
       " 716: 499,\n",
       " 717: 523,\n",
       " 718: 517,\n",
       " 719: 703,\n",
       " 720: 696,\n",
       " 721: -24825,\n",
       " 722: -24643,\n",
       " 723: -24549,\n",
       " 724: -24578,\n",
       " 725: -24581,\n",
       " 726: -24824,\n",
       " 727: -24416,\n",
       " 728: 1097,\n",
       " 729: 839,\n",
       " 730: 368,\n",
       " 731: 401,\n",
       " 732: 389,\n",
       " 733: 491,\n",
       " 734: 76,\n",
       " 735: 25,\n",
       " 736: -64,\n",
       " 737: 1007,\n",
       " 738: 947,\n",
       " 739: 953,\n",
       " 740: 1284,\n",
       " 741: 1716,\n",
       " 742: 1864,\n",
       " 743: 1317,\n",
       " 744: 744,\n",
       " 745: 817,\n",
       " 746: 804,\n",
       " 747: 630,\n",
       " 748: -954,\n",
       " 749: -1373,\n",
       " 750: 100,\n",
       " 751: 228,\n",
       " 752: 242,\n",
       " 753: 226,\n",
       " 754: -752,\n",
       " 755: 616,\n",
       " 756: 311,\n",
       " 757: 396,\n",
       " 758: 502,\n",
       " 759: 436,\n",
       " 760: 438,\n",
       " 761: 1230,\n",
       " 762: 1313,\n",
       " 763: -19,\n",
       " 764: -347,\n",
       " 765: -509,\n",
       " 766: -212,\n",
       " 767: -202}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#print(US_Deaths_Cases_df[((US_Deaths_Cases_df['Days'] == 221))]['Cumulative_Deaths'].sum())\n",
    "#print(US_Deaths_All_FIPS_df[((US_Deaths_All_FIPS_df['Days'] == 214))]['Cumulative_Deaths'].sum())\n",
    "#np.set_printoptions(suppress=True)\n",
    "#print(np.setdiff1d(US_Deaths_All_FIPS_df['FIPS'], US_Deaths_Cases_df['FIPS']))\n",
    "unassigned_fips = [  0.0, 60.0,    66.0,    69.0,    78.0,  2013.0,  2261.0, 72001.0,  72003.0,  72005.0,\n",
    " 72007.0,  72009.0,  72011.0,  72013.0,  72015.0,  72017.0,  72019.0,  72021.0,  72023.0,  72025.0,\n",
    " 72027.0,  72029.0,  72031.0,  72033.0,  72035.0,  72037.0,  72039.0,  72041.0,  72043.0,  72045.0,\n",
    " 72047.0,  72049.0,  72051.0,  72053.0,  72054.0,  72055.0,  72057.0,  72059.0,  72061.0,  72063.0,\n",
    " 72065.0,  72067.0,  72069.0,  72071.0,  72073.0,  72075.0,  72077.0,  72079.0,  72081.0,  72083.0,\n",
    " 72085.0,  72087.0,  72089.0,  72091.0,  72093.0,  72095.0,  72097.0,  72099.0,  72101.0,  72103.0,\n",
    " 72105.0,  72107.0,  72109.0,  72111.0,  72113.0,  72115.0,  72117.0,  72119.0,  72121.0,  72123.0,\n",
    " 72125.0,  72127.0,  72129.0,  72131.0,  72133.0,  72135.0,  72137.0,  72139.0,  72141.0,  72143.0,\n",
    " 72145.0,  72147.0,  72149.0,  72151.0,  72153.0,  72888.0,  72999.0,  80001.0, 80002.0, 80004.0, 80005.0,\n",
    " 80006.0, 80008.0, 80009.0, 80010.0, 80011.0, 80012.0, 80013.0, 80015.0, 80016.0, 80017.0,\n",
    " 80018.0, 80019.0, 80020.0, 80021.0, 80022.0, 80023.0, 80024.0, 80025.0, 80026.0, 80027.0,\n",
    " 80028.0, 80029.0, 80030.0, 80031.0, 80032.0, 80033.0, 80034.0, 80035.0, 80036.0, 80037.0,\n",
    " 80038.0, 80039.0, 80040.0, 80041.0, 80042.0, 80044.0, 80045.0, 80046.0, 80047.0, 80048.0,\n",
    " 80049.0, 80050.0, 80051.0, 80053.0, 80054.0, 80055.0, 80056.0, 88888.0, 90001.0, 90002.0,\n",
    " 90004.0, 90005.0, 90006.0, 90008.0, 90009.0, 90010.0, 90011.0, 90012.0, 90013.0, 90015.0,\n",
    " 90016.0, 90017.0, 90018.0, 90019.0, 90020.0, 90021.0, 90022.0, 90023.0, 90024.0, 90025.0,\n",
    " 90026.0, 90027.0, 90028.0, 90029.0, 90030.0, 90031.0, 90032.0, 90033.0, 90034.0, 90035.0,\n",
    " 90036.0, 90037.0, 90038.0, 90039.0, 90040.0, 90041.0, 90042.0, 90044.0, 90045.0, 90046.0,\n",
    " 90047.0, 90048.0, 90049.0, 90050.0, 90051.0, 90053.0, 90054.0, 90055.0, 90056.0, 99999.0]\n",
    "unassigned_deaths_df = US_Deaths_All_FIPS_df[US_Deaths_All_FIPS_df['FIPS'].isin(unassigned_fips)]\n",
    "US_unassigned_deaths = unassigned_deaths_df.groupby(['Province_State', 'Days'],as_index=False).agg({'Cumulative_Deaths': np.sum})\n",
    "US_unassigned_weekly_deaths = unassigned_deaths_df.groupby(['Province_State', 'Days'],as_index=False).agg({'Weekly_Deaths': np.sum})\n",
    "unassigned_deaths_by_days = unassigned_deaths_df.groupby(['Days'],as_index=False).agg({'Cumulative_Deaths': np.sum})\n",
    "unassigned_weekly_deaths_by_days = unassigned_deaths_df.groupby(['Days'],as_index=False).agg({'Weekly_Deaths': np.sum})\n",
    "US_unassigned_deaths_by_days = dict(zip(unassigned_deaths_by_days.Days,unassigned_deaths_by_days.Cumulative_Deaths))\n",
    "US_unassigned_weekly_deaths_by_days = dict(zip(unassigned_weekly_deaths_by_days.Days,unassigned_weekly_deaths_by_days.Weekly_Deaths))\n",
    "US_unassigned_weekly_deaths_by_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "def run_model(train_start_week_day,train_end_week_day,predict_start_week_day,predict_end_week_day,\n",
    "              covariates_merged,US_Deaths_Cases_df,predict_column_index,best_fit_scores):\n",
    "    predict_start_date = (covid_start_date + timedelta(days=predict_start_week_day)).date()\n",
    "    predict_end_date =  (covid_start_date + timedelta(days=predict_end_week_day)).date()\n",
    "    print(\"train:\", train_start_week_day ,\":\", train_end_week_day, \n",
    "        \" predict:\" , predict_start_week_day, \":\",  predict_end_week_day,\n",
    "        \" predict start date: \",  predict_start_date,\n",
    "        \" predict end date: \",  predict_end_date)\n",
    "    print \n",
    "    #print (\"training week number: \" , week_num+1)\n",
    "    #********************TRAIN MODEL ************************ \n",
    "    #train with all coutnies variates for one  week  and predict for LA county cumulative deaths\n",
    "    train_week = pd.Series(range(train_start_week_day,train_end_week_day))\n",
    "    #print (\"training week series :\" , train_week.array)\n",
    "    covariates_train_week = covariates_merged.loc[(covariates_merged['Days'].isin(train_week))] \n",
    "    US_Weekly_Cases_Deaths = US_Deaths_Cases_df.loc[(US_Deaths_Cases_df['Days'].isin(train_week))].iloc[:, predict_column_index]  \n",
    "    if len(US_Weekly_Cases_Deaths) > 0:\n",
    "       \n",
    "        X_train = covariates_train_week\n",
    "        Y_train = US_Weekly_Cases_Deaths\n",
    "        #reg = LassoCV(cv=5, random_state=0).fit(X_train, Y_train)\n",
    "        xgbReg = xgb.XGBRegressor()\n",
    "\n",
    "        parameters = {\n",
    "              'objective':['reg:squarederror'],\n",
    "              'learning_rate': [0.01, 0.05, 0.08], #so called `eta` value\n",
    "              'max_depth': [3,4],\n",
    "              'min_child_weight': [1,2],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree' : [0.8],\n",
    "              'n_estimators': [ 100,200]}\n",
    "        tscv = TimeSeriesSplit(n_splits=4)\n",
    "        xgb_grid = GridSearchCV(xgbReg,\n",
    "                        parameters,\n",
    "                        cv = tscv,\n",
    "                        n_jobs = 4,\n",
    "                        scoring = 'r2',\n",
    "                        verbose=True)\n",
    "\n",
    "        xgb_grid.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "        print(xgb_grid.best_score_)\n",
    "        print(xgb_grid.best_params_)\n",
    "        results = xgb_grid.cv_results_\n",
    "        \n",
    "        \n",
    "        predict_week = pd.Series(range(predict_start_week_day, predict_end_week_day ))\n",
    "        #print (\"predicting next week series :\" , predict_week.array)\n",
    "\n",
    "        X_predict_week = covariates_merged.loc[(covariates_merged['Days'].isin(predict_week))]\n",
    "        predict_week_df = US_Deaths_Cases_df.loc[(US_Deaths_Cases_df['Days'].isin(predict_week))]\n",
    "        Y_actual_predict_week = predict_week_df.iloc[:,predict_column_index]\n",
    "        #best_reg.fit(X_tune_week, Y_actual_tune_week)\n",
    "        Y_predict_week = xgb_grid.best_estimator_.predict(X_predict_week)\n",
    "        \n",
    "        #explainer = shap.Explainer(xgb_grid.best_estimator_)\n",
    "        #shap_values = explainer(X_predict_week)\n",
    "        #raw_shap_values = explainer.shap_values(X_predict_week)\n",
    "    \n",
    "        # visualize the first prediction's explanation\n",
    "        #shap.plots.waterfall(shap_values[0])\n",
    "        #shap.plots.beeswarm(shap_values)\n",
    "        #shap.plots.bar(shap_values)\n",
    "       \n",
    "        #vals= raw_shap_values.mean(0)\n",
    "        #print(vals)\n",
    "        #feature_names = X_predict_week.columns\n",
    "        #feature_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                  #columns=['col_name','feature_importance_vals'])\n",
    "        #feature_importance.to_csv(\"feature_importance.csv\", index=False)\n",
    "\n",
    "        #explainer = shap.TreeExplainer(xgb_grid.best_estimator_)\n",
    "        #shap_values = explainer.shap_values(X_predict_week)\n",
    "\n",
    "        # visualize the first prediction's explanation\n",
    "        \n",
    "        #shap.plots.waterfall(shap_values)\n",
    "        #shap.summary_plot(shap_values, X_predict_week)\n",
    "        \n",
    "        predict_score = r2_score(Y_actual_predict_week, Y_predict_week)\n",
    "        print(week_num , \":\", predict_score)\n",
    "        best_fit_scores.loc[index] = [str(predict_start_date) + ' - '  + str(predict_end_date), xgb_grid.best_params_, xgb_grid.best_score_,  predict_score]\n",
    "\n",
    "        return Y_predict_week\n",
    "    return None\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "WghtKSQ6xAYp",
    "outputId": "8b2721d6-8f6d-4812-84ef-7592ea4dda37",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_day:  0  max day:  795  number of weeks :  113\n",
      "train: 726 : 768  predict: 768 : 796\n",
      "train: 726 : 768  predict: 768 : 796  predict start date:  2022-02-28  predict end date:  2022-03-28\n",
      "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done  96 out of  96 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4953412360537902\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'objective': 'reg:squarederror', 'subsample': 0.8}\n",
      "0 : 0.6287440385881616\n",
      "train: 726 : 768  predict: 768 : 796  predict start date:  2022-02-28  predict end date:  2022-03-28\n",
      "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=4)]: Done  96 out of  96 | elapsed:  6.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7974907540281504\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 200, 'objective': 'reg:squarederror', 'subsample': 0.8}\n",
      "0 : 0.4782520128364358\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import shap\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV,TimeSeriesSplit\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "covid_start_date = datetime.strptime(\"01/22/20\", \"%m/%d/%y\")\n",
    "\n",
    "min_day=US_Deaths_Cases_df['Days'].min()\n",
    "max_day=US_Deaths_Cases_df['Days'].max()\n",
    "countyFIPS = US_Deaths_Cases_df[\"FIPS\"].unique()\n",
    "\n",
    "num_weeks=int((max_day-min_day)/7)\n",
    "print(\"min_day: \" , min_day, \" max day: \", max_day, \" number of weeks : \" , num_weeks)\n",
    "#prediction_days_arr = [7,14,21,28]\n",
    "#prediction_days = 7   # one week  train , tune and forecast\n",
    "#prediction_days = 14 # two weks train , tune and forecast\n",
    "#prediction_days = 21 # three weks train , tune and forecast\n",
    "prediction_days = 42 # four weeks train , tune and forecast\n",
    "\n",
    "best_fit_scores = pd.DataFrame(columns=[\"Week Duration\", \"Best_Parameters\", \"Best_Score\", \"Predict_R2_Score\"])\n",
    "predicted_df_all_days = pd.DataFrame(columns=['State','FIPS', 'Population','Forecast_Day','Days','Predicted_Weekly_Deaths' 'Predicted_Cumulative_Deaths','Predicted_Weekly_Cases' 'Predicted_Cumulative_Cases''Past_Week_Cumulative_Deaths'])\n",
    "\n",
    "train_start_week_day = model_day_offset-21\n",
    "train_end_week_day = train_start_week_day + prediction_days\n",
    "\n",
    "predict_start_week_day = train_end_week_day \n",
    "predict_end_week_day = predict_start_week_day + 28\n",
    "\n",
    "print(\"train:\", train_start_week_day ,\":\", train_end_week_day, \n",
    "        \" predict:\" , predict_start_week_day, \":\",  predict_end_week_day)\n",
    "\n",
    "for week_num in range(num_weeks):\n",
    "    if (predict_end_week_day-1) > max_day:\n",
    "        break\n",
    "    predicted_df = pd.DataFrame(columns=['State','FIPS','Population', 'Forecast_Day','Days','Predicted_Weekly_Deaths', 'Predicted_Weekly_Cases', 'Predicted_Cumulative_Deaths', 'Predicted_Cumulative_Cases','Past_Week_Cumulative_Deaths'])        \n",
    "    predicted_deaths = run_model(train_start_week_day,train_end_week_day,predict_start_week_day,predict_end_week_day,\n",
    "              covariates_merged,US_Deaths_Cases_df,5,best_fit_scores) \n",
    "    predicted_cases = run_model(train_start_week_day,train_end_week_day,predict_start_week_day,predict_end_week_day,\n",
    "              covariates_merged,US_Deaths_Cases_df,10,best_fit_scores) \n",
    "    predict_week = pd.Series(range(predict_start_week_day, predict_end_week_day ))\n",
    "    predict_week_df = US_Deaths_Cases_df.loc[(US_Deaths_Cases_df['Days'].isin(predict_week))]\n",
    "    predicted_df[\"State\"] = predict_week_df[\"STNAME\"]\n",
    "    predicted_df[\"FIPS\"] = predict_week_df[\"FIPS\"]\n",
    "    predicted_df[\"Population\"] = predict_week_df[\"Population\"]\n",
    "    predicted_df[\"Days\"] = predict_week_df[\"Days\"]\n",
    "    forecast_day = (covid_start_date + timedelta(days=(predict_start_week_day-1))).strftime(\"%Y-%m-%d\")\n",
    "    predicted_df[\"Forecast_Day\"] = forecast_day \n",
    "    predicted_df[\"Predicted_Weekly_Deaths\"] = predicted_deaths\n",
    "    predicted_df[\"Predicted_Weekly_Cases\"]  = predicted_cases\n",
    "                                         \n",
    "    predicted_df[\"Predicted_Cumulative_Deaths\"] = predicted_df[\"Predicted_Weekly_Deaths\"] + predict_week_df[\"Past_Week_Cumulative_Deaths\"]\n",
    "    predicted_df[\"Predicted_Cumulative_Cases\"] = predicted_df[\"Predicted_Weekly_Cases\"] + predict_week_df[\"Past_Week_Cumulative_Cases\"]\n",
    "    predicted_df['Past_Week_Cumulative_Deaths'] = predict_week_df[\"Past_Week_Cumulative_Deaths\"]\n",
    "    pwcdArray = predicted_df[\"Predicted_Cumulative_Deaths\"].to_numpy()\n",
    "    pwccArray = predicted_df[\"Predicted_Cumulative_Cases\"].to_numpy()\n",
    "    fipsArray = predicted_df['FIPS'].to_numpy()\n",
    "    j = 0\n",
    "    i = 0 \n",
    "    FIPS = 0.0\n",
    "    for index, row in predicted_df.iterrows():\n",
    "        countyFIPS = row['FIPS']\n",
    "        if  FIPS == countyFIPS:\n",
    "            if (j > 6) and  fipsArray[i-7] == countyFIPS:\n",
    "                past_week_cum_deaths = pwcdArray[i-7]\n",
    "                past_week_cum_cases= pwccArray[i-7]\n",
    "                predicted_df.at[index, 'Predicted_Cumulative_Deaths'] = (row[\"Predicted_Weekly_Deaths\"]) + past_week_cum_deaths\n",
    "                predicted_df.at[index, 'Predicted_Cumulative_Cases'] = (row[\"Predicted_Weekly_Cases\"]) + past_week_cum_cases\n",
    "                predicted_df.at[index,'Past_Week_Cumulative_Deaths'] = past_week_cum_deaths                \n",
    "                pwcdArray[i] = predicted_df.at[index, 'Predicted_Cumulative_Deaths']\n",
    "                pwccArray[i] = predicted_df.at[index, 'Predicted_Cumulative_Cases']\n",
    "                \n",
    "        else:\n",
    "            FIPS = row['FIPS']\n",
    "            j = 0 \n",
    "        i = i + 1\n",
    "        j = j + 1\n",
    "    predicted_df.to_csv(\"data/predictions/predicted_us_deaths_\" +  forecast_day + \".csv\")                     \n",
    "    predicted_df_all_days = pd.concat([predicted_df_all_days, predicted_df])\n",
    "\n",
    "    train_start_week_day = train_start_week_day + 7\n",
    "    train_end_week_day = train_end_week_day + 7\n",
    "    predict_start_week_day = train_end_week_day\n",
    "    predict_end_week_day = predict_start_week_day + 28\n",
    "   \n",
    "   \n",
    "predicted_df_all_days.to_csv(\"predicted_all_days.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_between(d1, d2):\n",
    "    d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n",
    "    d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n",
    "    return abs((d2 - d1).days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "S7woMnPEfGvn",
    "outputId": "2c83fed8-edf8-439a-aa78-b21440f8a386"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-02-27']\n",
      "2022-02-27\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "#predicted_df_all_days = pd.read_csv(\"predicted_all_days.csv\")\n",
    "covid_start_date = datetime.strptime(\"01/22/20\", \"%m/%d/%y\")\n",
    "\n",
    "state_predicted_deaths = predicted_df_all_days.groupby(['State', 'Days', 'Forecast_Day'], as_index=False)['Predicted_Cumulative_Deaths','Predicted_Weekly_Deaths'].sum()\n",
    "state_to_fips=pd.read_csv('https://docs.google.com/spreadsheets/d/1w4sHgYifJV-C8J1WV5rpTzEFLuyZ5ESp8r_1ck-9hlA/export?format=csv')\n",
    "state_predicted_deaths = state_predicted_deaths.merge(state_to_fips, how='left', left_on=['State'], right_on=['Name'])\n",
    "state_predicted_deaths=state_predicted_deaths.drop(columns=['Name'])\n",
    "country_predicted_deaths = predicted_df_all_days.groupby(['Days', 'Forecast_Day'], as_index=False)['Predicted_Cumulative_Deaths','Predicted_Weekly_Deaths'].sum()\n",
    "                        \n",
    "\n",
    "state_predicted_deaths=state_predicted_deaths[['State','FIPS','Days','Forecast_Day','Predicted_Cumulative_Deaths','Predicted_Weekly_Deaths']]\n",
    "iteration = 0  \n",
    "forecast_dates = country_predicted_deaths[\"Forecast_Day\"].unique()\n",
    "print(forecast_dates)\n",
    "for forecast_date in forecast_dates:\n",
    "    print(forecast_date)  \n",
    "    covid_hub_predicted_deaths = pd.DataFrame(columns=['forecast_date','target', 'target_end_date','location','type', 'quantile', 'value']) \n",
    "    filename = \"forecast-hub/data-processed/MIT_CritData-GBCF/{}-MIT_CritData-GBCF.csv\".format(forecast_date)\n",
    "    cp_detahs_forecast= country_predicted_deaths[country_predicted_deaths[\"Forecast_Day\"] == forecast_date]\n",
    "    max_day = US_unassigned_deaths['Days'].max()  \n",
    "    for index, row in cp_detahs_forecast.iterrows():\n",
    "\n",
    "      #predicted_df[\"FIPS\"] = predict_week_df[\"FIPS\"]\n",
    "      #predicted_df[\"Days\"] = predict_week_df[\"Days\"]\n",
    "      target_date = (covid_start_date + timedelta(days=row[\"Days\"]))\n",
    "  \n",
    "      if target_date.weekday() == 5:\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"location\"] = 'US'\n",
    "          #covid_hub_predicted_deaths.loc[iteration,\"location_name\"] = 'US'\n",
    "\n",
    "          forecast_day =  (datetime.strptime(row[\"Forecast_Day\"],\"%Y-%m-%d\") - covid_start_date).days\n",
    "          days_from = str(row[\"Days\"] - forecast_day)\n",
    "          actual_week =  (int(days_from) + 1)/7\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target\"] = str(int(actual_week)) + \" wk ahead cum death\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target_end_date\"] = target_date.strftime(\"%Y-%m-%d\")\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"forecast_date\"] =  row[\"Forecast_Day\"]\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"type\"] =\"point\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"quantile\"] =\"NA\"\n",
    "          if row[\"Predicted_Cumulative_Deaths\"] >= 0:\n",
    "\n",
    "            if row['Days'] in US_unassigned_deaths_by_days.keys():\n",
    "                covid_hub_predicted_deaths.loc[iteration,\"value\"] = int(round(row[\"Predicted_Cumulative_Deaths\"] + US_unassigned_deaths_by_days[row['Days']-7]))\n",
    "            else:\n",
    "                covid_hub_predicted_deaths.loc[iteration,\"value\"] = int(round(row[\"Predicted_Cumulative_Deaths\"] + US_unassigned_deaths_by_days[max_day]))\n",
    "          else:\n",
    "            covid_hub_predicted_deaths.loc[iteration,\"value\"] = 0\n",
    "\n",
    "          iteration = iteration + 1\n",
    "\n",
    "    for index, row in cp_detahs_forecast.iterrows():\n",
    "\n",
    "      #predicted_df[\"FIPS\"] = predict_week_df[\"FIPS\"]\n",
    "      #predicted_df[\"Days\"] = predict_week_df[\"Days\"]\n",
    "      target_date = (covid_start_date + timedelta(days=row[\"Days\"]))\n",
    "      if target_date.weekday() == 5:\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"location\"] = 'US'\n",
    "          #covid_hub_predicted_deaths.loc[iteration,\"location_name\"] = 'US'\n",
    "\n",
    "          forecast_day =  (datetime.strptime(row[\"Forecast_Day\"],\"%Y-%m-%d\") - covid_start_date).days\n",
    "          days_from = str(row[\"Days\"] - forecast_day)\n",
    "          actual_week =  (int(days_from) + 1)/7\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target\"] = str(int(actual_week)) + \" wk ahead inc death\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target_end_date\"] = target_date.strftime(\"%Y-%m-%d\")\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"forecast_date\"] =  row[\"Forecast_Day\"]\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"type\"] =\"point\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"quantile\"] =\"NA\"\n",
    "          if row[\"Predicted_Cumulative_Deaths\"] >= 0:\n",
    "\n",
    "            if row['Days'] in US_unassigned_deaths_by_days.keys():\n",
    "                covid_hub_predicted_deaths.loc[iteration,\"value\"] = int(round(row[\"Predicted_Weekly_Deaths\"] +  US_unassigned_weekly_deaths_by_days[row['Days']])) \n",
    "            else:\n",
    "                covid_hub_predicted_deaths.loc[iteration,\"value\"] = int(round(row[\"Predicted_Weekly_Deaths\"] +  US_unassigned_weekly_deaths_by_days[max_day])) \n",
    "          else:\n",
    "            covid_hub_predicted_deaths.loc[iteration,\"value\"] = 0\n",
    "\n",
    "          iteration = iteration + 1\n",
    "\n",
    "    state_predicted_deaths_forecast = state_predicted_deaths[state_predicted_deaths[\"Forecast_Day\"] == forecast_date]\n",
    "    for index, row in state_predicted_deaths_forecast.iterrows():\n",
    "\n",
    "      target_date = (covid_start_date + timedelta(days=row[\"Days\"]))\n",
    "      if target_date.weekday() == 5:\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"location\"] = str(row[\"FIPS\"]).zfill(2)\n",
    "          #covid_hub_predicted_deaths.loc[iteration,\"location_name\"] = row[\"State\"]\n",
    "\n",
    "          forecast_day =  (datetime.strptime(row[\"Forecast_Day\"],\"%Y-%m-%d\") - covid_start_date).days\n",
    "          days_from = str(row[\"Days\"] - forecast_day)\n",
    "          actual_week =  (int(days_from) + 1)/7\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target\"] = str(int(actual_week)) + \" wk ahead cum death\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target_end_date\"] = target_date.strftime(\"%Y-%m-%d\")\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"forecast_date\"] =  row[\"Forecast_Day\"]\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"type\"] =\"point\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"quantile\"] =\"NA\"\n",
    "          if row[\"Predicted_Cumulative_Deaths\"] >= 0:\n",
    "                days = row['Days']\n",
    "                max_day = US_unassigned_deaths['Days'].max()\n",
    "                if (days <= max_day):                \n",
    "                    covid_hub_predicted_deaths.loc[iteration,\"value\"] = int(round(row[\"Predicted_Cumulative_Deaths\"] + US_unassigned_deaths[(US_unassigned_deaths['Province_State'] == row['State']) & (US_unassigned_deaths['Days'] == row['Days']-7)]['Cumulative_Deaths'].values[0]))\n",
    "                else:\n",
    "                    covid_hub_predicted_deaths.loc[iteration,\"value\"] = int(round(row[\"Predicted_Cumulative_Deaths\"] + US_unassigned_deaths[(US_unassigned_deaths['Province_State'] == row['State']) & (US_unassigned_deaths['Days'] == max_day)]['Cumulative_Deaths'].values[0]))       \n",
    "          else:\n",
    "            covid_hub_predicted_deaths.loc[iteration,\"value\"] = 0\n",
    "          iteration = iteration + 1\n",
    "\n",
    "    for index, row in state_predicted_deaths_forecast.iterrows():\n",
    "\n",
    "      target_date = (covid_start_date + timedelta(days=row[\"Days\"]))\n",
    "      if target_date.weekday() == 5:\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"location\"] = str(row[\"FIPS\"]).zfill(2)\n",
    "          #covid_hub_predicted_deaths.loc[iteration,\"location_name\"] = row[\"State\"]\n",
    "\n",
    "          forecast_day =  (datetime.strptime(row[\"Forecast_Day\"],\"%Y-%m-%d\") - covid_start_date).days\n",
    "          days_from = str(row[\"Days\"] - forecast_day)\n",
    "          actual_week =  (int(days_from) + 1)/7\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target\"] = str(int(actual_week)) + \" wk ahead inc death\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target_end_date\"] = target_date.strftime(\"%Y-%m-%d\")\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"forecast_date\"] =  row[\"Forecast_Day\"]\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"type\"] =\"point\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"quantile\"] =\"NA\"\n",
    "          if row[\"Predicted_Cumulative_Deaths\"] >= 0:\n",
    "                days = row['Days']\n",
    "                max_day = US_unassigned_deaths['Days'].max()\n",
    "                if (days <= max_day):                \n",
    "                    wd = int(round(row[\"Predicted_Weekly_Deaths\"] + US_unassigned_weekly_deaths[(US_unassigned_weekly_deaths['Province_State'] == row['State']) & (US_unassigned_weekly_deaths['Days'] == row['Days'])]['Weekly_Deaths'].values[0]))\n",
    "                else:\n",
    "                    wd = int(round(row[\"Predicted_Weekly_Deaths\"] + US_unassigned_weekly_deaths[(US_unassigned_weekly_deaths['Province_State'] == row['State']) & (US_unassigned_weekly_deaths['Days'] == max_day)]['Weekly_Deaths'].values[0]))       \n",
    "                if wd < 0:\n",
    "                        covid_hub_predicted_deaths.loc[iteration,\"value\"] = 0\n",
    "                else:\n",
    "                    covid_hub_predicted_deaths.loc[iteration,\"value\"] = wd    \n",
    "          else:\n",
    "            covid_hub_predicted_deaths.loc[iteration,\"value\"] = 0\n",
    "          iteration = iteration + 1\n",
    "\n",
    "quantiles_ratios_wk1 = {0.01:0.9974368062,\n",
    "0.025:0.9975734937,\n",
    "0.05:0.9978553362,\n",
    "0.1:0.9982774951,\n",
    "0.15:0.9987383619,\n",
    "0.2:0.9989585135,\n",
    "0.25:0.9990528642,\n",
    "0.3:0.9994097034,\n",
    "0.35:0.9995935663,\n",
    "0.4:0.9997375115,\n",
    "0.45:0.9998741991,\n",
    "0.5:1,\n",
    "0.55:1.000084674,\n",
    "0.6:1.000270956,\n",
    "0.65:1.000402805,\n",
    "0.7:1.000506833,\n",
    "0.75:1.000654407,\n",
    "0.8:1.000749967,\n",
    "0.85:1.000864881,\n",
    "0.9:1.000971328,\n",
    "0.95:1.001527755,\n",
    "0.975:1.00167412,\n",
    "0.99:1.002918823}\n",
    "\n",
    "quantiles_ratios_wk2 = {0.01:0.996709557,\n",
    "0.025:0.9970841759,\n",
    "0.05:0.9972225702,\n",
    "0.1:0.9977081432,\n",
    "0.15:0.997990897,\n",
    "0.2:0.9983798326,\n",
    "0.25:0.9986387253,\n",
    "0.3:0.9988534751,\n",
    "0.35:0.9991958816,\n",
    "0.4:0.999491759,\n",
    "0.45:0.9997542309,\n",
    "0.5:1,\n",
    "0.55:1.000081128,\n",
    "0.6:1.000243383,\n",
    "0.65:1.000342407,\n",
    "0.7:1.000415183,\n",
    "0.75:1.00051182,\n",
    "0.8:1.00076952,\n",
    "0.85:1.000974725,\n",
    "0.9:1.00137917,\n",
    "0.95:1.002127215,\n",
    "0.975:1.002444568,\n",
    "0.99:1.00271897}\n",
    "\n",
    "quantiles_ratios_wk3 =  {0.01:0.995665615,\n",
    "0.025:0.9960219142,\n",
    "0.05:0.9963335289,\n",
    "0.1:0.9969649898,\n",
    "0.15:0.9973259926,\n",
    "0.2:0.9980797475,\n",
    "0.25:0.9986865144,\n",
    "0.3:0.9989146399,\n",
    "0.35:0.9992521246,\n",
    "0.4:0.9995261104,\n",
    "0.45:0.9996742744,\n",
    "0.5:1,\n",
    "0.55:1.000164627,\n",
    "0.6:1.000418622,\n",
    "0.65:1.00066909,\n",
    "0.7:1.000921909,\n",
    "0.75:1.001199423,\n",
    "0.8:1.001598055,\n",
    "0.85:1.001919077,\n",
    "0.9:1.002329467,\n",
    "0.95:1.003171415,\n",
    "0.975:1.003499493,\n",
    "0.99:1.004127426}\n",
    "\n",
    "quantiles_ratios_wk4 = {0.01:0.9952033895,\n",
    "0.025:0.9961583078,\n",
    "0.05:0.9965778229,\n",
    "0.1:0.9970239923,\n",
    "0.15:0.9974098999,\n",
    "0.2:0.9978641814,\n",
    "0.25:0.9981411541,\n",
    "0.3:0.9984667998,\n",
    "0.35:0.9987588379,\n",
    "0.4:0.9993950639,\n",
    "0.45:0.9997914013,\n",
    "0.5:1,\n",
    "0.55:1.000409085,\n",
    "0.6:1.000616525,\n",
    "0.65:1.000701123,\n",
    "0.7:1.000865684,\n",
    "0.75:1.0010569,\n",
    "0.8:1.001402247,\n",
    "0.85:1.001788154,\n",
    "0.9:1.002209987,\n",
    "0.95:1.002858961,\n",
    "0.975:1.003598327,\n",
    "0.99:1.003983076}\n",
    "\n",
    "qw_ratios = {1.0:quantiles_ratios_wk1, 2.0:quantiles_ratios_wk2, 3.0:quantiles_ratios_wk3,4.0:quantiles_ratios_wk4}\n",
    "weeks=[1,2,3,4]\n",
    "quantiles=[0.01,0.025,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.975,0.99]\n",
    "covid_hub_predicted_deaths_all = covid_hub_predicted_deaths.copy()\n",
    "for index, row in covid_hub_predicted_deaths.iterrows():\n",
    "        wk = (days_between(row['forecast_date'],row['target_end_date']) + 1) / 7\n",
    "        qr = qw_ratios[wk]\n",
    "        for quantile in quantiles:\n",
    "            covid_hub_predicted_deaths_all.loc[iteration,\"location\"] = row['location']\n",
    "            covid_hub_predicted_deaths_all.loc[iteration,\"target\"] = row['target']\n",
    "            covid_hub_predicted_deaths_all.loc[iteration,\"target_end_date\"] =row['target_end_date']\n",
    "            covid_hub_predicted_deaths_all.loc[iteration,\"forecast_date\"] =  row[\"forecast_date\"]\n",
    "            covid_hub_predicted_deaths_all.loc[iteration,\"type\"] =\"quantile\"\n",
    "            covid_hub_predicted_deaths_all.loc[iteration,\"quantile\"] = quantile\n",
    "            covid_hub_predicted_deaths_all.loc[iteration,\"value\"] = math.ceil(row['value'] * qr[quantile])\n",
    "            iteration = iteration + 1\n",
    "\n",
    "covid_hub_predicted_deaths_all.to_csv(filename,index = False)\n",
    "  #files.download(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import evaluate_models\n",
    "from datetime import datetime, timedelta\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "files = ['2020-08-30-MIT_CritData-GBCF.csv'] #listdir('forecast-hub/data-processed/MIT_CritData-GBCF')\n",
    "files.sort()\n",
    "for f in files :\n",
    "    if f!= '.DS_Store':\n",
    "        proj_date = datetime.strptime(f[:10],\"%Y-%m-%d\").date() +  timedelta(days=1) #Monday\n",
    "        eval_date = proj_date +  timedelta(days=5) #Saturday\n",
    "\n",
    "        print(\"running evaluation for proj_date: \", proj_date, \" eval_date: \" , eval_date)\n",
    "        evaluate_models.run_evaluation('forecast-hub',proj_date, eval_date, \"eval/one-week\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for f in files :\n",
    "    if f!= '.DS_Store':\n",
    "        proj_date = datetime.strptime(f[:10],\"%Y-%m-%d\").date() +  timedelta(days=1) #Monday\n",
    "        eval_date = proj_date +  timedelta(days=12) #Saturday\n",
    "\n",
    "        print(\"running evaluation for proj_date: \", proj_date, \" eval_date: \" , eval_date)\n",
    "        evaluate_models.run_evaluation('forecast-hub',proj_date, eval_date, \"eval/two-weeks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import shap\n",
    "\n",
    "# train an XGBoost model\n",
    "X, y = shap.datasets.boston()\n",
    "model = xgboost.XGBRegressor().fit(X, y)\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X)\n",
    "print(shap_values.data)\n",
    "# visualize the first prediction's explanation\n",
    "shap.plots.waterfall(shap_values[0])\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Forecaster v2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
